{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfdd8489",
   "metadata": {},
   "source": [
    "> **Topic:** Introduction to Neural Network using PyTorch\n",
    ">\n",
    "> **Module:** TensorBoard, Checkpoint, K-Fold cross validation\n",
    ">\n",
    "> **Presentor:** Industry Sandbox and AI Computing (ISAIC)\n",
    ">\n",
    "> **Date:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ad692",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "Tensorboard is a visualization tool from `tensorflow` to track metrics while running machine learning model. We can visualize as many as we want (e.g. loss, accuracy) in real time while the model is training.\n",
    "\n",
    "We need to launch tensorboard in jupyter notebook before running the training process in order the get real-time updates on the metrics.\n",
    "\n",
    "Before going into tensorboard, let's reload everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53d4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytorch modules\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "#from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#import numpy module\n",
    "import numpy as np\n",
    "\n",
    "#import plotting modules\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rc('figure', dpi=350)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "pre_processing = transforms.Compose(\n",
    "                [transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5])])\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "                    root='FMNIST',\n",
    "                    train=True,\n",
    "                    download=False,\n",
    "                    transform=pre_processing)\n",
    "test_data  = datasets.FashionMNIST(\n",
    "                    root='FMNIST',\n",
    "                    train=False,\n",
    "                    download=False,\n",
    "                    transform=pre_processing)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_data,  batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88fbe5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load necessary modules\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#define the model class\n",
    "class ImageClass(nn.Module):\n",
    "    '''\n",
    "    Main class that defines the Neural Net model (inheritence from nn.Module)\n",
    "    Input - input_dim (int): Input dimension for each sample data\n",
    "            hidden_dim (list): Each element in the list denotes the dimension\n",
    "                            of each hidden layer\n",
    "            output_dim (int): Output dimension (in our example case, 10 for \n",
    "                            total 10 classes of image)\n",
    "            dropout_rate (float) [optional]: Rate at which Dropout regularization is applied\n",
    "            use_batchnorm (bool) [optional]: Whether to use batch normalization after each\n",
    "                                            hidden layer affine transformation\n",
    "            **kwargs: Additional keyword arguments\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=[64,64,64], output_dim=10,\n",
    "                dropout_rate = 0.0, use_batchnorm=False, **kwargs):\n",
    "        super(ImageClass, self).__init__(**kwargs)\n",
    "        \n",
    "        #define an empty ModuleList container\n",
    "        self.linear_model = nn.ModuleList()\n",
    "        \n",
    "        #first flatten our 2D image data into one dimension\n",
    "        self.linear_model.append(nn.Flatten())\n",
    "        \n",
    "        #then we build our hidden layers iteratively (based on the # hidden layers)\n",
    "        for i, (in_channel, out_channel) in enumerate(\n",
    "                                zip([input_dim]+hidden_dim[:-1], hidden_dim)):\n",
    "            #we first build the affine transformation\n",
    "            self.linear_model.append(nn.Linear(in_channel, out_channel, bias=True))\n",
    "            if use_batchnorm:\n",
    "                #then apply batch normalization, if turned on for the model\n",
    "                self.linear_model.append(nn.BatchNorm1d(out_channel))\n",
    "            #we then apply the activation function\n",
    "            self.linear_model.append(nn.ReLU())\n",
    "            if dropout_rate:\n",
    "                #we also add dropout is this regularization is turned on\n",
    "                self.linear_model.append(nn.Dropout(dropout_rate))\n",
    "        #add the last layer, i.e. the model output\n",
    "        self.linear_model.append(nn.Linear(hidden_dim[-1], output_dim, bias=True))\n",
    "        self.linear_model.append(nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for layer in self.linear_model:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f3f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary python modules (provides the optimizer library)\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "######  Hyperparameter settings  #######\n",
    "\n",
    "#Number of epochs to train for\n",
    "N_EPOCHS = 20\n",
    "\n",
    "#define the loss function, i.e. negative log-likelihood function (acting on log-Softmax model output)\n",
    "loss_function = F.nll_loss\n",
    "\n",
    "#Initialize the model with set architecture\n",
    "input_dim = 28*28 #pixel size for our dataset\n",
    "hidden_dim = [64,64,64] #List of dimensions in each hidden layer\n",
    "output_dim = 10 #dimension of the output layer\n",
    "\n",
    "#Regularization parameter\n",
    "dropout_rate=0.3\n",
    "use_batchnorm=True\n",
    "\n",
    "#initialize the model\n",
    "model = ImageClass(input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                  dropout_rate=dropout_rate, use_batchnorm=use_batchnorm)\n",
    "\n",
    "#****IMPORTANT: move the model to GPU (if allowed), before constructing optimizer\n",
    "#set the proper device (use GPU to train on if available)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "#set the learning rate to use for the optimizer\n",
    "learning_rate = 0.001\n",
    "#initialize the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "#momentum = 0.9\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46d4e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def evaluate(model, dataloader, loss_function):\n",
    "    model.eval()\n",
    "    y_true = np.zeros(len(dataloader.dataset))\n",
    "    y_pred = np.zeros(len(dataloader.dataset))\n",
    "    \n",
    "    total_loss=0\n",
    "    \n",
    "    for batch_idx, (X,y) in enumerate(dataloader):\n",
    "        print(f'\\rEvaluating {batch_idx + 1} / {len(dataloader)}', end='\\r')\n",
    "        y_i = model(X)\n",
    "        loss = loss_function(y_i, y)\n",
    "        idx_start = batch_idx*dataloader.batch_size\n",
    "        idx_stop  = (batch_idx+1)*dataloader.batch_size\n",
    "        y_true[idx_start:idx_stop] = y.data.numpy()\n",
    "        y_pred[idx_start:idx_stop] = y_i.data.max(dim=1)[1].numpy()\n",
    "        total_loss+= loss.item()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    loss = total_loss/len(dataloader)\n",
    "    print (f'Running Metrics: Accuracy Score: {acc}, Loss: {loss}')\n",
    "    return (y_true, y_pred, acc, loss)\n",
    "\n",
    "def train(model, train_loader, loss_function, optimizer, nepoch=20, book_tb=False, tb_writer=None):\n",
    "    for epoch in range(nepoch):\n",
    "        #print the running epch number\n",
    "        print (f'\\nEpoch {epoch+1}/{nepoch}')\n",
    "        #set the model to training mode\n",
    "        model.train()\n",
    "        #variable to calculate running loss (for each epoch)\n",
    "        epoch_loss = 0\n",
    "        epoch_accu = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            X, y = data\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = loss_function(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_accu += accuracy_score(y.data.numpy(),y_pred.data.max(dim=1)[1].numpy())\n",
    "        #print the model performance using test dataset\n",
    "        print ('Validation###')\n",
    "        _, _, test_accu, test_loss = evaluate(model, test_loader, loss_function)\n",
    "        \n",
    "        if book_tb:\n",
    "            epoch_loss /= len(train_loader)\n",
    "            tb_writer[0].add_scalar(\"Loss\", epoch_loss, epoch+1)\n",
    "            \n",
    "            epoch_accu /= (batch_idx+1)\n",
    "            tb_writer[0].add_scalar(\"Accuracy\", epoch_accu, epoch+1)\n",
    "            \n",
    "            tb_writer[1].add_scalar(\"Loss\", test_loss, epoch+1)\n",
    "            tb_writer[1].add_scalar(\"Accuracy\", test_accu, epoch+1)\n",
    "    return None\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5bc148",
   "metadata": {},
   "source": [
    "- **Launching TensorBoard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39e3912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 22097), started 0:02:02 ago. (Use '!kill 22097' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9639ed5fbe5ba6d8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9639ed5fbe5ba6d8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorboard\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b6586",
   "metadata": {},
   "source": [
    "To visualize the metrics, we need to first initialize a summary writer.\n",
    "\n",
    "We then add our metrics in the writer object to view in the tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c46be3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8535, Loss: 0.40758793591596065\n",
      "\n",
      "Epoch 2/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.856, Loss: 0.3979589608721078\n",
      "\n",
      "Epoch 3/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8614, Loss: 0.3809892482841358\n",
      "\n",
      "Epoch 4/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8593, Loss: 0.3770973405089622\n",
      "\n",
      "Epoch 5/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8613, Loss: 0.3768467872192304\n",
      "\n",
      "Epoch 6/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8681, Loss: 0.36850538842689495\n",
      "\n",
      "Epoch 7/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8675, Loss: 0.36696951465008737\n",
      "\n",
      "Epoch 8/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.87, Loss: 0.3637193247747307\n",
      "\n",
      "Epoch 9/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.872, Loss: 0.3535024970293807\n",
      "\n",
      "Epoch 10/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8707, Loss: 0.3567437940655044\n",
      "\n",
      "Epoch 11/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8741, Loss: 0.3478111807768718\n",
      "\n",
      "Epoch 12/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8733, Loss: 0.346994489871751\n",
      "\n",
      "Epoch 13/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8739, Loss: 0.34922591434976163\n",
      "\n",
      "Epoch 14/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8757, Loss: 0.34683315265483367\n",
      "\n",
      "Epoch 15/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8717, Loss: 0.34887406030021156\n",
      "\n",
      "Epoch 16/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8746, Loss: 0.34237194277893623\n",
      "\n",
      "Epoch 17/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8738, Loss: 0.3426053092692988\n",
      "\n",
      "Epoch 18/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8765, Loss: 0.34535827785254286\n",
      "\n",
      "Epoch 19/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8755, Loss: 0.3440092089838875\n",
      "\n",
      "Epoch 20/20\n",
      "Validation###\n",
      "Running Metrics: Accuracy Score: 0.8764, Loss: 0.3415098968928995\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "train_writer = SummaryWriter(comment=\"Training\")\n",
    "test_writer  = SummaryWriter(comment=\"Testing\")\n",
    "\n",
    "train(model, train_loader, loss_function, optimizer, book_tb=True, tb_writer=[train_writer, test_writer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d06b9",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "When the model is training through epochs iteratively, we can save each instance of the model as checkpoints and load it later from desired checkpoint either to resume training process or to investigate the model from particular epoch.\n",
    "\n",
    "The following script shows the general saving and loading model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49a72ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving a check point\n",
    "# Additional information\n",
    "EPOCH = 20\n",
    "PATH = \"model.pt\"\n",
    "LOSS = 0.3963\n",
    "\n",
    "torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "551b2909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClass(\n",
       "  (linear_model): ModuleList(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=64, bias=True)\n",
       "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "    (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): Dropout(p=0.3, inplace=False)\n",
       "    (13): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (14): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading a checkpoint\n",
    "\n",
    "#initialize the model\n",
    "model = ImageClass(input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                  dropout_rate=dropout_rate, use_batchnorm=use_batchnorm)\n",
    "\n",
    "#****IMPORTANT: move the model to GPU (if allowed), before constructing optimizer\n",
    "#set the proper device (use GPU to train on if available)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "#set the learning rate to use for the optimizer\n",
    "learning_rate = 0.001\n",
    "#initialize the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "# - or -\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316e6b7",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0034146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import ConcatDataset, SubsetRandomSampler\n",
    "\n",
    "k_folds = 5\n",
    "\n",
    "dataset = ConcatDataset([train_data, test_data])\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=10, sampler=train_subsampler)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=10, sampler=test_subsampler)\n",
    "    train(model, train_loader, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc885d",
   "metadata": {},
   "source": [
    "We can save model checkpoints after the end of training for each fold and load it later to compare the model performance across all K-folds to choose the best performing model for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a4736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df37447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
